#!/usr/bin/env python3


class Argument:
    def __init__(self, name, cuda_type, num_registers, const=False, is_array=True, ptx_type=None, is_braced=True):
        self.name = name
        self.cuda_type = cuda_type
        self.num_registers = num_registers
        self.const = const
        self.is_array = is_array
        self.ptx_type = ptx_type
        self.is_braced = is_braced

        
    def make_cuda_arglist(self):
        if self.is_array:
            cv = 'const ' if self.const else ''
            return [ f'{cv}{self.cuda_type} {self.name}[{self.num_registers}]' ]

        ret = [ ]
        for i in range(self.num_registers):
            cv = '' if self.const else '&'
            ret.append(f'{cuda_type} {cv}{self.name}{i}')
        
        return ret


    def make_ptx_argstr(self, base):
        t = [ f'%{i}' for i in range(base, base + self.num_registers) ]
        t = ', '.join(t)
        if self.is_braced:
            t =  '{' + t + '}'
        return t


    def make_constraint_str(self):
        ret = [ ]
        for i in range(self.num_registers):
            v = f'{self.name}[{i}]' if self.is_array else f'{self.name}{i}'
            if self.ptx_type is not None:
                cv = 'const ' if self.const else ''
                v = f'*({cv}{self.ptx_type} *) &{v}'
                
            constraint = '"r"' if self.const else '"=r"'
            ret.append(f'{constraint} ({v})')

        return ', '.join(ret)


####################################################################################################


def emit_kernel(cuda_name, ptx_name, *args):
    cuda_arglist = [ ]
    for arg in args:
        cuda_arglist += arg.make_cuda_arglist()

    cuda_argstr = ', '.join(cuda_arglist)
    
    print(f'')
    print(f'// D = A*B + C')
    print(f'__device__ __forceinline__')
    print(f'void {cuda_name}({cuda_argstr})')
    print(f'{{')
    print(f'    asm("{ptx_name} "')

    base = 0
    for i,arg in enumerate(args):
        s = f'"{arg.make_ptx_argstr(base)}'
        t = ', "' if (i < len(args)-1) else ';" :'
        base += arg.num_registers
        print(f'        {s}{t}')

    for i,arg in enumerate(args):
        t = ''
        if i == 0:
            t = ' :'
        elif i < len(args)-1:
            t = ','

        s = arg.make_constraint_str()
        print(f'        {s}{t}')
    
    print(f'    );')
    print(f'}}')
    print(f'')


####################################################################################################

    
def emit_dense_mma(cuda_name, ptx_name, cuda_type, dbits, sbits, m, n, k, ptx_type=None):
    # Register counts
    na = (m*k*sbits) // 1024
    nb = (k*n*sbits) // 1024
    nc = (m*n*dbits) // 1024

    emit_kernel(
        cuda_name,
        ptx_name,
        Argument('d', cuda_type, nc, ptx_type=ptx_type),
        Argument('a', cuda_type, na, ptx_type=ptx_type, const=True),
        Argument('b', cuda_type, nb, ptx_type=ptx_type, const=True),
        Argument('c', cuda_type, nc, ptx_type=ptx_type, const=True)
    )


def emit_dense_f16_mma(m, n, k):
    cuda_name = f'mma_f16_m{m}_n{n}_k{k}'
    ptx_name = f'mma.sync.aligned.m{m}n{n}k{k}.row.col.f16.f16.f16.f16'
    emit_dense_mma(cuda_name, ptx_name, '__half2', 16, 16, m, n, k, ptx_type='unsigned int')


def emit_dense_int_mma(sbits, m, n, k):
    cuda_name = f'mma_s{sbits}_m{m}_n{n}_k{k}'
    ptx_name = f'mma.sync.aligned.m{m}n{n}k{k}.row.col.satfinite.s32.s{sbits}.s{sbits}.s32'
    emit_dense_mma(cuda_name, ptx_name, 'int', 32, sbits, m, n, k)
    

####################################################################################################

    
if __name__ == '__main__':
    print(f'#ifndef _GPUTILS_DEVICE_MMA_HPP')
    print(f'#define _GPUTILS_DEVICE_MMA_HPP')
    print(f'')
    print(f'// Autogenerated by generate_device_mma_hpp.py')
    print(f'//')
    print(f'// Reference for matrix shapes:')
    print(f'//   https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-shape')
    print(f'//')
    print(f'// Reference for PTX instruction syntax:')
    print(f'//   https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-mma')
    print()
    print('#include <cuda_fp16.h>')
    print(f'')
    print(f'namespace gputils {{')
    print(f'')

    emit_dense_f16_mma(16, 8, 8)
    emit_dense_f16_mma(16, 8, 16)
    
    emit_dense_int_mma(4, 8, 8, 32)
    emit_dense_int_mma(4, 16, 8, 32)
    emit_dense_int_mma(4, 16, 8, 64)

    emit_dense_int_mma(8, 8, 8, 16)
    emit_dense_int_mma(8, 16, 8, 16)
    emit_dense_int_mma(8, 16, 8, 32)
    
    print(f'')
    print(f'}} // namespace gputils')
    print(f'')
    print(f'#endif // _GPUTILS_DEVICE_MMA_HPP')

